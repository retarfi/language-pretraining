{
    "bert-small" : {
        "number-of-layers" : 12,
        "hidden-size" : 256,
        "sequence-length" : 128,
        "ffn-inner-hidden-size" : 1024,
        "attention-heads" : 4,
        "warmup-steps" : 10000,
        "learning-rate" : 5e-4,
        "batch-size" : {
            "-1" : 128
        },
        "train-steps" : 1450000,
        "save-steps" : 100000,
        "fp16-type": 0,
        "bf16": false
    },
    "bert-base" : {
        "number-of-layers" : 12,
        "hidden-size" : 768,
        "sequence-length" : 512,
        "ffn-inner-hidden-size" : 3072,
        "attention-heads" : 12,
        "warmup-steps" : 10000,
        "learning-rate" : 1e-4,
        "batch-size" : {
            "-1" : 256
        },
        "train-steps" : 1000000,
        "fp16-type": 0,
        "bf16": false
    },
    "bert-large" : {
        "number-of-layers" : 24,
        "hidden-size" : 1024,
        "sequence-length" : 512,
        "ffn-inner-hidden-size" : 4096,
        "attention-heads" : 16,
        "warmup-steps" : 10000,
        "learning-rate" : 1e-4,
        "batch-size" : {
            "-1" : 256
        },
        "train-steps" : 464000,
        "fp16-type": 0,
        "bf16": false
    },
    "electra-small-paper" : {
        "number-of-layers" : 12,
        "hidden-size" : 256,
        "sequence-length" : 128,
        "ffn-inner-hidden-size" : 1024,
        "attention-heads" : 4,
        "embedding-size" : 128,
        "generator-size" : "1/4",
        "mask-percent" : 15,
        "warmup-steps" : 10000,
        "learning-rate" : 5e-4,
        "batch-size" : {
            "-1" : 128
        },
        "train-steps" : 1000000,
        "save-steps" : 100000,
        "fp16-type": 0,
        "bf16": false
    },
    "electra-small" : {
        "number-of-layers" : 12,
        "hidden-size" : 256,
        "sequence-length" : 128,
        "ffn-inner-hidden-size" : 1024,
        "attention-heads" : 4,
        "embedding-size" : 128,
        "generator-size" : "1/1",
        "mask-percent" : 15,
        "warmup-steps" : 10000,
        "learning-rate" : 5e-4,
        "batch-size" : {
            "-1" : 128
        },
        "train-steps" : 1000000,
        "save-steps" : 100000,
        "fp16-type": 0,
        "bf16": false
    },
    "electra-base" : {
        "number-of-layers" : 12,
        "hidden-size" : 768,
        "sequence-length" : 512,
        "ffn-inner-hidden-size" : 3072,
        "attention-heads" : 12,
        "embedding-size" : 768,
        "generator-size" : "1/3",
        "mask-percent" : 15,
        "warmup-steps" : 10000,
        "learning-rate" : 2e-4,
        "batch-size" : {
            "-1" : 256
        },
        "train-steps" : 766000,
        "fp16-type": 0,
        "bf16": false
    },
    "electra-large" : {
        "number-of-layers" : 24,
        "hidden-size" : 1024,
        "sequence-length" : 512,
        "ffn-inner-hidden-size" : 4096,
        "attention-heads" : 16,
        "embedding-size" : 1024,
        "generator-size" : "1/4",
        "mask-percent" : 25,
        "warmup-steps" : 10000,
        "learning-rate" : 2e-4,
        "batch-size" : {
            "-1" : 2048
        },
        "train-steps" : 400000,
        "fp16-type": 0,
        "bf16": false
    },
    "roberta-base" : {
        "number-of-layers" : 12,
        "hidden-size" : 768,
        "sequence-length" : 512,
        "ffn-inner-hidden-size" : 3072,
        "attention-heads" : 12,
        "warmup-steps" : 24000,
        "learning-rate" : 6e-4,
        "batch-size" : {
            "-1" : 8000
        },
        "train-steps" : 500000,
        "save-steps" : 50000,
        "logging-steps" : 5000,
        "fp16-type": 0,
        "bf16": false
    },
    "roberta-large" : {
        "number-of-layers" : 24,
        "hidden-size" : 1024,
        "sequence-length" : 512,
        "ffn-inner-hidden-size" : 4096,
        "attention-heads" : 16,
        "warmup-steps" : 30000,
        "learning-rate" : 4e-4,
        "batch-size" : {
            "-1" : 8000
        },
        "train-steps" : 500000,
        "save-steps" : 50000,
        "logging-steps" : 5000,
        "fp16-type": 0,
        "bf16": false
    },
    "bert-base-dist" : {
        "number-of-layers" : 12,
        "hidden-size" : 768,
        "sequence-length" : 512,
        "ffn-inner-hidden-size" : 3072,
        "attention-heads" : 12,
        "warmup-steps" : 10000,
        "learning-rate" : 1e-4,
        "batch-size" : {
            "0" : 80,
            "1" : 80,
            "2" : 48,
            "3" : 48
        },
        "train-steps" : 1000000,
        "save-steps" : 50000,
        "logging-steps" : 5000,
        "fp16-type": 0,
        "bf16": false
    },
    "bert-small-additional" : {
        "pretrained_model_name_or_path" : "izumi-lab/bert-small-japanese",
        "flozen-layers" : 6,
        "warmup-steps" : 10000,
        "learning-rate" : 1e-4,
        "batch-size" : {
            "-1" : 128
        },
        "train-steps" : 1450000,
        "save-steps" : 100000,
        "fp16-type": 0,
        "bf16": false
    },
    "electra-small-additional" : {
        "pretrained_generator_model_name_or_path" : "izumi-lab/electra-small-paper-japanese-generator",
        "pretrained_discriminator_model_name_or_path" : "izumi-lab/electra-small-paper-japanese-discriminator",
        "flozen-layers" : -1,
        "warmup-steps" : 10000,
        "learning-rate" : 1e-4,
        "batch-size" : {
            "-1" : 128
        },
        "train-steps" : 1000000,
        "save-steps" : 100000,
        "fp16-type": 0,
        "bf16": false
    }
}
